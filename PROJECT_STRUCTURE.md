# Project Structure

```
llm-inference/
â”‚
â”œâ”€â”€ ğŸ“± FRONTEND (React)
â”‚   â”œâ”€â”€ public/                      # Static assets
â”‚   â”‚   â”œâ”€â”€ index.html              # Main HTML template
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â””â”€â”€ manifest.json
â”‚   â”‚
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                  # Main React component
â”‚   â”‚   â”‚                           # - Model selection dropdowns
â”‚   â”‚   â”‚                           # - Chat interface
â”‚   â”‚   â”‚                           # - Backend API integration
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ App.css                 # Styling
â”‚   â”‚   â”‚                           # - Gradient background
â”‚   â”‚   â”‚                           # - Side-by-side layout
â”‚   â”‚   â”‚                           # - Responsive design
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ index.js                # React entry point
â”‚   â”‚   â””â”€â”€ index.css               # Global styles
â”‚   â”‚
â”‚   â”œâ”€â”€ package.json                # npm dependencies
â”‚   â””â”€â”€ package-lock.json
â”‚
â”œâ”€â”€ ğŸ”§ BACKEND (FastAPI)
â”‚   â””â”€â”€ backend/
â”‚       â”œâ”€â”€ main.py                 # FastAPI server
â”‚       â”‚                           # - TGI container management
â”‚       â”‚                           # - Docker integration
â”‚       â”‚                           # - API endpoints
â”‚       â”‚                           # - Model lifecycle management
â”‚       â”‚
â”‚       â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚       â”‚                           # - fastapi
â”‚       â”‚                           # - uvicorn
â”‚       â”‚                           # - docker
â”‚       â”‚                           # - httpx
â”‚       â”‚
â”‚       â”œâ”€â”€ test_api.py            # API testing script
â”‚       â””â”€â”€ README.md              # Backend documentation
â”‚
â”œâ”€â”€ ğŸ“š DOCUMENTATION
â”‚   â”œâ”€â”€ README.md                   # Main project documentation
â”‚   â”œâ”€â”€ QUICKSTART.md              # Getting started guide
â”‚   â”œâ”€â”€ API_REFERENCE.md           # Complete API documentation
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md       # This file
â”‚
â”œâ”€â”€ ğŸš€ SCRIPTS
â”‚   â””â”€â”€ start.sh                    # Easy startup script
â”‚                                   # - Checks dependencies
â”‚                                   # - Starts backend & frontend
â”‚
â””â”€â”€ âš™ï¸ CONFIG
    â”œâ”€â”€ .gitignore                 # Git ignore rules
    â””â”€â”€ .env.example               # Environment variables template

```

## Component Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER BROWSER                          â”‚
â”‚                     http://localhost:3000                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  React App (src/App.js)                            â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  â€¢ Model Selection Dropdowns                       â”‚    â”‚
â”‚  â”‚  â€¢ Chat Interface                                  â”‚    â”‚
â”‚  â”‚  â€¢ Message History                                 â”‚    â”‚
â”‚  â”‚  â€¢ Status Indicators                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â”‚ HTTP Requests                    â”‚
â”‚                           â–¼                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ fetch('/generate', ...)
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                           â”‚
â”‚                  http://localhost:8000                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  main.py                                           â”‚    â”‚
â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Endpoints:                                        â”‚    â”‚
â”‚  â”‚  â€¢ GET  /models          (list available)         â”‚    â”‚
â”‚  â”‚  â€¢ POST /models/{id}/start  (start container)     â”‚    â”‚
â”‚  â”‚  â€¢ POST /models/{id}/stop   (stop container)      â”‚    â”‚
â”‚  â”‚  â€¢ POST /generate           (generate text)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â”‚ Docker Python SDK                â”‚
â”‚                           â–¼                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ docker.containers.run(...)
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Docker Engine                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  TGI Container  â”‚  â”‚  TGI Container  â”‚                  â”‚
â”‚  â”‚  (Model 1)      â”‚  â”‚  (Model 2)      â”‚  ...             â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚                  â”‚
â”‚  â”‚  GPT-2          â”‚  â”‚  TinyLlama      â”‚                  â”‚
â”‚  â”‚  Port: 8080     â”‚  â”‚  Port: 8081     â”‚                  â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚                  â”‚
â”‚  â”‚  /generate      â”‚  â”‚  /generate      â”‚                  â”‚
â”‚  â”‚  /health        â”‚  â”‚  /health        â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ Model weights cached at:
                            â–¼
                   ~/.cache/huggingface/
```

## Request Flow Example

1. **User** types "What is AI?" and selects GPT-2 & TinyLlama
2. **Frontend** sends two parallel POST requests to `/generate`
3. **Backend** receives requests:
   - Checks if containers are running
   - Starts containers if needed (downloads models first time)
   - Waits for containers to be ready
4. **Backend** forwards requests to TGI containers
5. **TGI Containers** generate text using the models
6. **Backend** returns generated text to frontend
7. **Frontend** displays both responses side-by-side

## Key Files Explained

### Frontend

**`src/App.js`** (176 lines)
- State management for models, messages, loading states
- API integration with backend
- Message handling and display logic
- Model selection UI

**`src/App.css`** (290 lines)
- Modern gradient design
- Responsive grid layout
- Smooth animations
- Mobile-friendly breakpoints

### Backend

**`backend/main.py`** (250+ lines)
- FastAPI application setup
- Docker client integration
- Container lifecycle management
- TGI communication
- Error handling

**`backend/requirements.txt`**
- Production-ready dependencies
- Minimal and focused

### Documentation

**`README.md`**
- Complete project overview
- Installation instructions
- Usage guide
- Troubleshooting

**`QUICKSTART.md`**
- Step-by-step getting started
- Common issues and solutions
- First-time user guide

**`API_REFERENCE.md`**
- Complete API documentation
- Request/response examples
- Error codes
- Model specifications

## Technologies Used

### Frontend
- **React 18** - UI framework
- **Fetch API** - HTTP requests
- **CSS3** - Modern styling with gradients and animations
- **Hooks** - useState, useEffect for state management

### Backend
- **FastAPI** - Modern Python web framework
- **Uvicorn** - ASGI server
- **Docker SDK** - Container management
- **HTTPX** - Async HTTP client
- **Pydantic** - Data validation

### Infrastructure
- **Docker** - Container runtime
- **HuggingFace TGI** - Text generation inference
- **HuggingFace Hub** - Model repository

## Model Storage

Models are cached locally to avoid re-downloading:

```
~/.cache/huggingface/
â”œâ”€â”€ hub/
â”‚   â”œâ”€â”€ models--gpt2/
â”‚   â”œâ”€â”€ models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/
â”‚   â””â”€â”€ models--mistralai--Mistral-7B-Instruct-v0.1/
â””â”€â”€ ...
```

## Port Usage

- **3000**: React development server
- **8000**: FastAPI backend server
- **8080-8089**: TGI containers (dynamically allocated)

## Development vs Production

### Development (Current Setup)
- Frontend: React dev server (hot reload)
- Backend: Uvicorn with auto-reload
- Models: Downloaded on-demand

### Production Considerations
- Frontend: Build with `npm run build`, serve with nginx
- Backend: Use gunicorn with multiple workers
- Models: Pre-download and cache
- Add authentication and rate limiting
- Use environment variables for configuration
- Implement proper logging and monitoring

## Extending the Application

### Add a New Model

1. Edit `backend/main.py`:
```python
MODELS = {
    "my-model": "organization/model-name-on-hf",
}
```

2. Frontend will automatically fetch and display it!

### Add Model Configuration UI

Add to `src/App.js`:
- Temperature slider
- Max tokens input
- Top-p slider

### Add Streaming Responses

Modify backend to use TGI's streaming endpoint and frontend to handle Server-Sent Events (SSE).

### Add Authentication

Add middleware in backend:
```python
from fastapi.security import HTTPBearer
```

## Performance Considerations

### Memory Usage
- GPT-2: ~2GB RAM
- TinyLlama: ~4GB RAM  
- 7B models: ~16GB RAM

### Optimization Tips
1. Use smaller models for development
2. Limit concurrent containers
3. Implement container pooling
4. Add request queuing
5. Cache frequent prompts

## Security Notes

âš ï¸ **This is a development setup**

For production:
- Add authentication
- Implement rate limiting
- Validate all inputs
- Use HTTPS
- Secure Docker socket access
- Set resource limits on containers
- Add CORS restrictions
- Implement request timeouts

---

Built with â¤ï¸ using React, FastAPI, and HuggingFace TGI

